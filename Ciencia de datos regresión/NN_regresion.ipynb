{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25970d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"BarcelonaAbnb_limpio.csv\")\n",
    "df\n",
    "import numpy as np\n",
    "\n",
    "df[\"log_price\"] = np.log1p(df[\"price\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b36b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([210., 285., 170., ...,  90.,  90., 120.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reg = df[\"price\"].values\n",
    "y_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7992b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_response_time</th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>host_acceptance_rate</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>...</th>\n",
       "      <th>room_type_Shared room</th>\n",
       "      <th>neighbourhood_group_cleansed_Eixample</th>\n",
       "      <th>neighbourhood_group_cleansed_Gràcia</th>\n",
       "      <th>neighbourhood_group_cleansed_Horta-Guinardó</th>\n",
       "      <th>neighbourhood_group_cleansed_Les Corts</th>\n",
       "      <th>neighbourhood_group_cleansed_Nou Barris</th>\n",
       "      <th>neighbourhood_group_cleansed_Sant Andreu</th>\n",
       "      <th>neighbourhood_group_cleansed_Sant Martí</th>\n",
       "      <th>neighbourhood_group_cleansed_Sants-Montjuïc</th>\n",
       "      <th>neighbourhood_group_cleansed_Sarrià-Sant Gervasi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.405560</td>\n",
       "      <td>2.172620</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.412432</td>\n",
       "      <td>2.219750</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.405660</td>\n",
       "      <td>2.170150</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.380620</td>\n",
       "      <td>2.175170</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.379780</td>\n",
       "      <td>2.176230</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13410</th>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.382010</td>\n",
       "      <td>2.177120</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13411</th>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.410689</td>\n",
       "      <td>2.170413</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13412</th>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.390695</td>\n",
       "      <td>2.164782</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13413</th>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.396987</td>\n",
       "      <td>2.166294</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13414</th>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.380181</td>\n",
       "      <td>2.154822</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13415 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       host_response_time  host_response_rate  host_acceptance_rate  \\\n",
       "0                     0.0                96.0                  91.0   \n",
       "1                     0.0               100.0                  96.0   \n",
       "2                     0.0               100.0                 100.0   \n",
       "3                     0.0               100.0                  92.0   \n",
       "4                     0.0               100.0                  92.0   \n",
       "...                   ...                 ...                   ...   \n",
       "13410                 1.0               100.0                  98.0   \n",
       "13411                 1.0                87.0                   6.0   \n",
       "13412                 1.0                87.0                   6.0   \n",
       "13413                 1.0                87.0                   6.0   \n",
       "13414                 0.0                98.0                  98.0   \n",
       "\n",
       "       host_is_superhost  host_has_profile_pic  host_identity_verified  \\\n",
       "0                    0.0                   1.0                     1.0   \n",
       "1                    1.0                   1.0                     1.0   \n",
       "2                    0.0                   1.0                     1.0   \n",
       "3                    0.0                   1.0                     1.0   \n",
       "4                    0.0                   1.0                     1.0   \n",
       "...                  ...                   ...                     ...   \n",
       "13410                0.0                   1.0                     1.0   \n",
       "13411                0.0                   1.0                     1.0   \n",
       "13412                0.0                   1.0                     1.0   \n",
       "13413                0.0                   1.0                     1.0   \n",
       "13414                0.0                   1.0                     1.0   \n",
       "\n",
       "        latitude  longitude  accommodates  bathrooms  ...  \\\n",
       "0      41.405560   2.172620           8.0        2.0  ...   \n",
       "1      41.412432   2.219750           5.0        2.0  ...   \n",
       "2      41.405660   2.170150           6.0        2.0  ...   \n",
       "3      41.380620   2.175170           2.0        1.0  ...   \n",
       "4      41.379780   2.176230           9.0        3.0  ...   \n",
       "...          ...        ...           ...        ...  ...   \n",
       "13410  41.382010   2.177120           5.0        1.0  ...   \n",
       "13411  41.410689   2.170413           3.0        1.0  ...   \n",
       "13412  41.390695   2.164782           4.0        1.0  ...   \n",
       "13413  41.396987   2.166294           4.0        1.0  ...   \n",
       "13414  41.380181   2.154822           4.0        1.0  ...   \n",
       "\n",
       "       room_type_Shared room  neighbourhood_group_cleansed_Eixample  \\\n",
       "0                          0                                      1   \n",
       "1                          0                                      0   \n",
       "2                          0                                      0   \n",
       "3                          0                                      0   \n",
       "4                          0                                      0   \n",
       "...                      ...                                    ...   \n",
       "13410                      0                                      0   \n",
       "13411                      0                                      0   \n",
       "13412                      0                                      1   \n",
       "13413                      0                                      1   \n",
       "13414                      0                                      1   \n",
       "\n",
       "       neighbourhood_group_cleansed_Gràcia  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        1   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "...                                    ...   \n",
       "13410                                    0   \n",
       "13411                                    0   \n",
       "13412                                    0   \n",
       "13413                                    0   \n",
       "13414                                    0   \n",
       "\n",
       "       neighbourhood_group_cleansed_Horta-Guinardó  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "...                                            ...   \n",
       "13410                                            0   \n",
       "13411                                            1   \n",
       "13412                                            0   \n",
       "13413                                            0   \n",
       "13414                                            0   \n",
       "\n",
       "       neighbourhood_group_cleansed_Les Corts  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "...                                       ...   \n",
       "13410                                       0   \n",
       "13411                                       0   \n",
       "13412                                       0   \n",
       "13413                                       0   \n",
       "13414                                       0   \n",
       "\n",
       "       neighbourhood_group_cleansed_Nou Barris  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "...                                        ...   \n",
       "13410                                        0   \n",
       "13411                                        0   \n",
       "13412                                        0   \n",
       "13413                                        0   \n",
       "13414                                        0   \n",
       "\n",
       "       neighbourhood_group_cleansed_Sant Andreu  \\\n",
       "0                                             0   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "...                                         ...   \n",
       "13410                                         0   \n",
       "13411                                         0   \n",
       "13412                                         0   \n",
       "13413                                         0   \n",
       "13414                                         0   \n",
       "\n",
       "       neighbourhood_group_cleansed_Sant Martí  \\\n",
       "0                                            0   \n",
       "1                                            1   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "...                                        ...   \n",
       "13410                                        0   \n",
       "13411                                        0   \n",
       "13412                                        0   \n",
       "13413                                        0   \n",
       "13414                                        0   \n",
       "\n",
       "       neighbourhood_group_cleansed_Sants-Montjuïc  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "...                                            ...   \n",
       "13410                                            0   \n",
       "13411                                            0   \n",
       "13412                                            0   \n",
       "13413                                            0   \n",
       "13414                                            0   \n",
       "\n",
       "       neighbourhood_group_cleansed_Sarrià-Sant Gervasi  \n",
       "0                                                     0  \n",
       "1                                                     0  \n",
       "2                                                     0  \n",
       "3                                                     0  \n",
       "4                                                     0  \n",
       "...                                                 ...  \n",
       "13410                                                 0  \n",
       "13411                                                 0  \n",
       "13412                                                 0  \n",
       "13413                                                 0  \n",
       "13414                                                 0  \n",
       "\n",
       "[13415 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_excluir_reg = [\n",
    "    \"price\",\n",
    "    \"estimated_revenue_l365d\"\n",
    "    ,  \"log_price\"\n",
    "]\n",
    "\n",
    "X_reg = df.drop(columns=cols_excluir_reg)\n",
    "X_reg = X_reg.select_dtypes(include=[\"number\"])\n",
    "\n",
    "X_reg \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde8db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e9feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c68f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_regression_model(\n",
    "    n_features,\n",
    "    n_hidden1=128,\n",
    "    n_hidden2=64,\n",
    "    n_hidden3=32,\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=1e-3\n",
    "):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(n_features,)),\n",
    "        layers.Dense(n_hidden1, activation=\"relu\"),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(n_hidden2, activation=\"relu\"),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(n_hidden3, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"linear\") \n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d766ac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 7920.7202 - mae: 61.9420 - val_loss: 3294.4536 - val_mae: 41.5545\n",
      "Epoch 2/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3343.0090 - mae: 41.4257 - val_loss: 3047.8523 - val_mae: 39.8251\n",
      "Epoch 3/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3190.8374 - mae: 40.3063 - val_loss: 2945.4128 - val_mae: 39.2439\n",
      "Epoch 4/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3099.5886 - mae: 39.6020 - val_loss: 2870.8979 - val_mae: 38.7652\n",
      "Epoch 5/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3041.2659 - mae: 39.1992 - val_loss: 2845.3447 - val_mae: 38.1192\n",
      "Epoch 6/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2946.1748 - mae: 38.5999 - val_loss: 2841.6399 - val_mae: 38.1825\n",
      "Epoch 7/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2906.1726 - mae: 38.2551 - val_loss: 2755.5393 - val_mae: 37.9906\n",
      "Epoch 8/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2862.6997 - mae: 37.9783 - val_loss: 2743.7893 - val_mae: 37.5710\n",
      "Epoch 9/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2856.2012 - mae: 38.1739 - val_loss: 2677.7991 - val_mae: 36.9412\n",
      "Epoch 10/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2823.5225 - mae: 37.9097 - val_loss: 2695.7356 - val_mae: 37.4231\n",
      "Epoch 11/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2783.5361 - mae: 37.5512 - val_loss: 2693.4807 - val_mae: 36.7534\n",
      "Epoch 12/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2769.2153 - mae: 37.3121 - val_loss: 2667.8455 - val_mae: 37.3697\n",
      "Epoch 13/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2767.1052 - mae: 37.3387 - val_loss: 2723.4175 - val_mae: 37.0147\n",
      "Epoch 14/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2723.6499 - mae: 37.2105 - val_loss: 2575.5701 - val_mae: 36.5065\n",
      "Epoch 15/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2623.8469 - mae: 36.3497 - val_loss: 2583.0327 - val_mae: 37.0162\n",
      "Epoch 16/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2656.8699 - mae: 36.5293 - val_loss: 2582.5789 - val_mae: 37.0217\n",
      "Epoch 17/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2620.9294 - mae: 36.4723 - val_loss: 2531.6807 - val_mae: 36.3575\n",
      "Epoch 18/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2614.2773 - mae: 36.2487 - val_loss: 2524.6990 - val_mae: 36.3138\n",
      "Epoch 19/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2549.3315 - mae: 35.8909 - val_loss: 2534.3799 - val_mae: 36.4513\n",
      "Epoch 20/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2564.8430 - mae: 36.0212 - val_loss: 2508.7598 - val_mae: 36.2071\n",
      "Epoch 21/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2583.1831 - mae: 36.1077 - val_loss: 2628.0491 - val_mae: 36.2003\n",
      "Epoch 22/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2539.3252 - mae: 35.7882 - val_loss: 2467.2510 - val_mae: 35.2556\n",
      "Epoch 23/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2507.8733 - mae: 35.6627 - val_loss: 2471.1658 - val_mae: 35.4025\n",
      "Epoch 24/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2526.2102 - mae: 35.6375 - val_loss: 2486.1367 - val_mae: 35.1112\n",
      "Epoch 25/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2465.1641 - mae: 35.3240 - val_loss: 2447.3669 - val_mae: 35.0495\n",
      "Epoch 26/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2495.5989 - mae: 35.4443 - val_loss: 2491.2705 - val_mae: 35.2333\n",
      "Epoch 27/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2461.2244 - mae: 35.3470 - val_loss: 2458.4128 - val_mae: 35.2213\n",
      "Epoch 28/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2464.7964 - mae: 35.3155 - val_loss: 2457.2522 - val_mae: 35.6583\n",
      "Epoch 29/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2443.5728 - mae: 34.9741 - val_loss: 2448.7847 - val_mae: 35.2449\n",
      "Epoch 30/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2462.7263 - mae: 35.2588 - val_loss: 2429.4797 - val_mae: 34.9262\n",
      "Epoch 31/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2461.4348 - mae: 35.3003 - val_loss: 2417.8477 - val_mae: 34.8372\n",
      "Epoch 32/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2419.8196 - mae: 34.9425 - val_loss: 2400.5630 - val_mae: 34.5414\n",
      "Epoch 33/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2360.6270 - mae: 34.6220 - val_loss: 2393.0222 - val_mae: 34.8527\n",
      "Epoch 34/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2350.0408 - mae: 34.5971 - val_loss: 2407.9146 - val_mae: 35.0389\n",
      "Epoch 35/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2371.0630 - mae: 34.4141 - val_loss: 2412.6704 - val_mae: 34.5295\n",
      "Epoch 36/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2347.9131 - mae: 34.3737 - val_loss: 2444.7932 - val_mae: 35.8068\n",
      "Epoch 37/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2328.2686 - mae: 34.3601 - val_loss: 2375.3657 - val_mae: 34.6620\n",
      "Epoch 38/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2329.5649 - mae: 34.3169 - val_loss: 2372.9023 - val_mae: 34.7873\n",
      "Epoch 39/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2302.5095 - mae: 34.1212 - val_loss: 2377.1294 - val_mae: 34.2800\n",
      "Epoch 40/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2253.6870 - mae: 33.8287 - val_loss: 2374.1890 - val_mae: 34.4502\n",
      "Epoch 41/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2304.7869 - mae: 34.1249 - val_loss: 2405.6624 - val_mae: 34.5427\n",
      "Epoch 42/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2279.3137 - mae: 33.8556 - val_loss: 2383.7209 - val_mae: 34.4387\n",
      "Epoch 43/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2263.1458 - mae: 33.9619 - val_loss: 2370.4165 - val_mae: 34.2640\n",
      "Epoch 44/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2253.4590 - mae: 33.7008 - val_loss: 2369.5349 - val_mae: 33.9816\n",
      "Epoch 45/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2236.8523 - mae: 33.5273 - val_loss: 2339.5159 - val_mae: 34.1972\n",
      "Epoch 46/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2225.6169 - mae: 33.6658 - val_loss: 2379.0681 - val_mae: 34.2888\n",
      "Epoch 47/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 2199.8933 - mae: 33.3027 - val_loss: 2349.5149 - val_mae: 34.6522\n",
      "Epoch 48/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 2190.1201 - mae: 33.4160 - val_loss: 2462.6948 - val_mae: 34.3660\n",
      "Epoch 49/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2187.8489 - mae: 33.3810 - val_loss: 2360.5005 - val_mae: 34.7000\n",
      "Epoch 50/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2188.1199 - mae: 33.3861 - val_loss: 2321.9180 - val_mae: 34.0493\n",
      "Epoch 51/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2209.3965 - mae: 33.3890 - val_loss: 2333.7571 - val_mae: 34.2370\n",
      "Epoch 52/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2159.9067 - mae: 33.1460 - val_loss: 2321.1951 - val_mae: 34.2503\n",
      "Epoch 53/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2187.2756 - mae: 33.2612 - val_loss: 2322.2847 - val_mae: 33.9622\n",
      "Epoch 54/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2147.2476 - mae: 32.9091 - val_loss: 2302.8215 - val_mae: 34.0456\n",
      "Epoch 55/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2128.8123 - mae: 33.0506 - val_loss: 2326.1223 - val_mae: 34.2778\n",
      "Epoch 56/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2116.5735 - mae: 32.8746 - val_loss: 2284.3445 - val_mae: 33.7990\n",
      "Epoch 57/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2076.6912 - mae: 32.4703 - val_loss: 2298.4233 - val_mae: 33.9808\n",
      "Epoch 58/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2135.4451 - mae: 32.7701 - val_loss: 2333.2107 - val_mae: 33.5969\n",
      "Epoch 59/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 2097.5264 - mae: 32.6721 - val_loss: 2293.6506 - val_mae: 33.5117\n",
      "Epoch 60/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - loss: 2078.1272 - mae: 32.4446 - val_loss: 2411.7053 - val_mae: 34.1161\n",
      "Epoch 61/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 2106.2378 - mae: 32.6564 - val_loss: 2279.0500 - val_mae: 33.5893\n",
      "Epoch 62/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2034.5977 - mae: 32.3618 - val_loss: 2390.3435 - val_mae: 35.3796\n",
      "Epoch 63/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2085.7678 - mae: 32.5995 - val_loss: 2271.4360 - val_mae: 33.6084\n",
      "Epoch 64/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2070.2329 - mae: 32.3786 - val_loss: 2311.1594 - val_mae: 33.4949\n",
      "Epoch 65/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2048.6726 - mae: 32.3871 - val_loss: 2261.6313 - val_mae: 33.3033\n",
      "Epoch 66/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2040.4900 - mae: 32.0698 - val_loss: 2294.6794 - val_mae: 34.1402\n",
      "Epoch 67/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2031.7534 - mae: 32.1773 - val_loss: 2281.7676 - val_mae: 33.3802\n",
      "Epoch 68/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2029.9119 - mae: 32.1422 - val_loss: 2270.7222 - val_mae: 33.1594\n",
      "Epoch 69/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2072.2407 - mae: 32.5178 - val_loss: 2303.4714 - val_mae: 33.7668\n",
      "Epoch 70/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1973.0656 - mae: 31.7935 - val_loss: 2275.9365 - val_mae: 33.3594\n",
      "Epoch 71/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1986.1182 - mae: 31.9546 - val_loss: 2295.1201 - val_mae: 33.6030\n",
      "Epoch 72/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1992.1501 - mae: 31.7088 - val_loss: 2314.6562 - val_mae: 33.4636\n",
      "Epoch 73/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1992.8889 - mae: 31.9876 - val_loss: 2272.3467 - val_mae: 33.2656\n",
      "Epoch 74/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1988.1783 - mae: 31.6405 - val_loss: 2264.6973 - val_mae: 33.3728\n",
      "Epoch 75/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1957.2247 - mae: 31.5946 - val_loss: 2305.3943 - val_mae: 33.5010\n",
      "Epoch 76/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1973.0341 - mae: 31.6332 - val_loss: 2290.6626 - val_mae: 33.2146\n",
      "Epoch 77/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1974.3317 - mae: 31.6215 - val_loss: 2288.9067 - val_mae: 33.3733\n",
      "Epoch 78/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1953.0052 - mae: 31.5069 - val_loss: 2284.1138 - val_mae: 33.4235\n",
      "Epoch 79/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1948.3394 - mae: 31.4836 - val_loss: 2360.8813 - val_mae: 33.6113\n",
      "Epoch 80/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1958.3893 - mae: 31.7029 - val_loss: 2251.2583 - val_mae: 33.1248\n",
      "Epoch 81/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1964.0499 - mae: 31.6503 - val_loss: 2389.8059 - val_mae: 33.8325\n",
      "Epoch 82/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1939.9933 - mae: 31.3328 - val_loss: 2259.5762 - val_mae: 32.8732\n",
      "Epoch 83/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1891.2166 - mae: 31.0726 - val_loss: 2342.1509 - val_mae: 33.5358\n",
      "Epoch 84/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1916.4894 - mae: 31.1604 - val_loss: 2332.9436 - val_mae: 33.4259\n",
      "Epoch 85/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1880.5909 - mae: 31.0349 - val_loss: 2249.4512 - val_mae: 32.8458\n",
      "Epoch 86/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1939.8542 - mae: 31.4210 - val_loss: 2264.5664 - val_mae: 33.1205\n",
      "Epoch 87/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1888.7007 - mae: 31.1364 - val_loss: 2267.8391 - val_mae: 33.0932\n",
      "Epoch 88/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1851.7625 - mae: 30.7805 - val_loss: 2249.4009 - val_mae: 32.7986\n",
      "Epoch 89/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1869.1392 - mae: 30.9638 - val_loss: 2273.1819 - val_mae: 32.8806\n",
      "Epoch 90/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1904.2686 - mae: 31.0448 - val_loss: 2244.9880 - val_mae: 33.0173\n",
      "Epoch 91/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1849.6873 - mae: 30.8857 - val_loss: 2336.6614 - val_mae: 33.3553\n",
      "Epoch 92/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1890.6401 - mae: 31.0658 - val_loss: 2389.5049 - val_mae: 33.7394\n",
      "Epoch 93/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1838.0084 - mae: 30.6894 - val_loss: 2356.3311 - val_mae: 33.4265\n",
      "Epoch 94/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1872.3994 - mae: 30.8276 - val_loss: 2251.3770 - val_mae: 32.8830\n",
      "Epoch 95/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1852.5757 - mae: 30.8349 - val_loss: 2255.1184 - val_mae: 32.8802\n",
      "Epoch 96/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1840.5380 - mae: 30.7829 - val_loss: 2406.6775 - val_mae: 33.6458\n",
      "Epoch 97/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1855.5160 - mae: 30.9019 - val_loss: 2301.7026 - val_mae: 33.1532\n",
      "Epoch 98/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1815.7538 - mae: 30.4637 - val_loss: 2329.4695 - val_mae: 33.3901\n",
      "Epoch 99/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1864.9573 - mae: 30.8493 - val_loss: 2420.5813 - val_mae: 33.9173\n",
      "Epoch 100/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1815.6077 - mae: 30.4089 - val_loss: 2272.0586 - val_mae: 32.8413\n"
     ]
    }
   ],
   "source": [
    "n_features_reg = X_train_reg_scaled.shape[1]\n",
    "\n",
    "model_reg = build_regression_model(n_features_reg)\n",
    "\n",
    "history_reg = model_reg.fit(\n",
    "    X_train_reg_scaled,\n",
    "    y_train_reg,\n",
    "    validation_data=(X_test_reg_scaled, y_test_reg),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2147148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "MSE: 2272.0591479683226\n",
      "RMSE: 47.66612159561886\n",
      "MAE: 32.84133217008621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "y_pred_reg = model_reg.predict(X_test_reg_scaled).flatten()\n",
    "\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2ec70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation (RMSE / Mean Price): 0.33078065807638457\n"
     ]
    }
   ],
   "source": [
    "df[\"price\"].describe()\n",
    "\n",
    "eval = rmse/df[\"price\"].mean()\n",
    "print(\"Evaluation (RMSE / Mean Price):\", eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9067ee5",
   "metadata": {},
   "source": [
    "Hicimos research y vimos que transformar logarítmicamente el precio estabiliza la varianza y reduce la influencia de outliers, lo que nos puede servir para predecir de mejor forma. Ahora predeciremos con base en esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7424a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg = df[\"log_price\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e357ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_excluir_reg = [\n",
    "    \"price\",\n",
    "    \"log_price\",\n",
    "    \"estimated_revenue_l365d\"  \n",
    "]\n",
    "\n",
    "X_reg = df.drop(columns=cols_excluir_reg)\n",
    "X_reg = X_reg.select_dtypes(include=[\"number\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4b033a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a42d48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4ce8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_regression_model(\n",
    "    n_features,\n",
    "    n_hidden1=128,\n",
    "    n_hidden2=64,\n",
    "    n_hidden3=32,\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=1e-3\n",
    "):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(n_features,)),\n",
    "        layers.Dense(n_hidden1, activation=\"relu\"),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(n_hidden2, activation=\"relu\"),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(n_hidden3, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00023b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.7833 - mae: 0.9432 - val_loss: 0.4588 - val_mae: 0.5462\n",
      "Epoch 2/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5272 - mae: 0.5732 - val_loss: 0.2895 - val_mae: 0.4217\n",
      "Epoch 3/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4050 - mae: 0.4993 - val_loss: 0.2569 - val_mae: 0.3977\n",
      "Epoch 4/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3334 - mae: 0.4548 - val_loss: 0.2470 - val_mae: 0.3986\n",
      "Epoch 5/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2786 - mae: 0.4178 - val_loss: 0.2831 - val_mae: 0.4411\n",
      "Epoch 6/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2460 - mae: 0.3909 - val_loss: 0.2358 - val_mae: 0.3952\n",
      "Epoch 7/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2115 - mae: 0.3620 - val_loss: 0.1650 - val_mae: 0.3153\n",
      "Epoch 8/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1923 - mae: 0.3430 - val_loss: 0.1683 - val_mae: 0.3207\n",
      "Epoch 9/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1738 - mae: 0.3254 - val_loss: 0.2613 - val_mae: 0.4226\n",
      "Epoch 10/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1657 - mae: 0.3168 - val_loss: 0.1737 - val_mae: 0.3286\n",
      "Epoch 11/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1545 - mae: 0.3059 - val_loss: 0.2031 - val_mae: 0.3633\n",
      "Epoch 12/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1490 - mae: 0.2985 - val_loss: 0.1355 - val_mae: 0.2808\n",
      "Epoch 13/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1421 - mae: 0.2915 - val_loss: 0.1459 - val_mae: 0.2977\n",
      "Epoch 14/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1376 - mae: 0.2867 - val_loss: 0.1442 - val_mae: 0.2949\n",
      "Epoch 15/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1361 - mae: 0.2847 - val_loss: 0.1272 - val_mae: 0.2680\n",
      "Epoch 16/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1312 - mae: 0.2788 - val_loss: 0.1290 - val_mae: 0.2740\n",
      "Epoch 17/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1291 - mae: 0.2776 - val_loss: 0.1260 - val_mae: 0.2678\n",
      "Epoch 18/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1260 - mae: 0.2737 - val_loss: 0.1463 - val_mae: 0.3006\n",
      "Epoch 19/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1254 - mae: 0.2721 - val_loss: 0.1368 - val_mae: 0.2856\n",
      "Epoch 20/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1232 - mae: 0.2700 - val_loss: 0.1202 - val_mae: 0.2615\n",
      "Epoch 21/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1203 - mae: 0.2667 - val_loss: 0.1199 - val_mae: 0.2595\n",
      "Epoch 22/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1197 - mae: 0.2658 - val_loss: 0.1198 - val_mae: 0.2621\n",
      "Epoch 23/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1136 - mae: 0.2595 - val_loss: 0.1181 - val_mae: 0.2587\n",
      "Epoch 24/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1128 - mae: 0.2583 - val_loss: 0.1169 - val_mae: 0.2586\n",
      "Epoch 25/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1102 - mae: 0.2544 - val_loss: 0.1276 - val_mae: 0.2727\n",
      "Epoch 26/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1081 - mae: 0.2517 - val_loss: 0.1190 - val_mae: 0.2650\n",
      "Epoch 27/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1063 - mae: 0.2500 - val_loss: 0.1138 - val_mae: 0.2525\n",
      "Epoch 28/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1081 - mae: 0.2519 - val_loss: 0.1093 - val_mae: 0.2443\n",
      "Epoch 29/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1021 - mae: 0.2441 - val_loss: 0.1199 - val_mae: 0.2649\n",
      "Epoch 30/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1032 - mae: 0.2454 - val_loss: 0.1143 - val_mae: 0.2556\n",
      "Epoch 31/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1012 - mae: 0.2426 - val_loss: 0.1195 - val_mae: 0.2613\n",
      "Epoch 32/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1019 - mae: 0.2427 - val_loss: 0.1131 - val_mae: 0.2516\n",
      "Epoch 33/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0984 - mae: 0.2400 - val_loss: 0.1106 - val_mae: 0.2469\n",
      "Epoch 34/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0977 - mae: 0.2383 - val_loss: 0.1168 - val_mae: 0.2564\n",
      "Epoch 35/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0954 - mae: 0.2358 - val_loss: 0.1235 - val_mae: 0.2691\n",
      "Epoch 36/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0951 - mae: 0.2344 - val_loss: 0.1197 - val_mae: 0.2616\n",
      "Epoch 37/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0936 - mae: 0.2334 - val_loss: 0.1139 - val_mae: 0.2526\n",
      "Epoch 38/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0940 - mae: 0.2332 - val_loss: 0.1089 - val_mae: 0.2462\n",
      "Epoch 39/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0915 - mae: 0.2313 - val_loss: 0.1108 - val_mae: 0.2445\n",
      "Epoch 40/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0912 - mae: 0.2299 - val_loss: 0.1128 - val_mae: 0.2498\n",
      "Epoch 41/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0908 - mae: 0.2297 - val_loss: 0.1116 - val_mae: 0.2487\n",
      "Epoch 42/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0877 - mae: 0.2263 - val_loss: 0.1143 - val_mae: 0.2530\n",
      "Epoch 43/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0858 - mae: 0.2241 - val_loss: 0.1194 - val_mae: 0.2637\n",
      "Epoch 44/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0874 - mae: 0.2249 - val_loss: 0.1122 - val_mae: 0.2495\n",
      "Epoch 45/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0860 - mae: 0.2238 - val_loss: 0.1079 - val_mae: 0.2441\n",
      "Epoch 46/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0845 - mae: 0.2215 - val_loss: 0.1119 - val_mae: 0.2495\n",
      "Epoch 47/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0854 - mae: 0.2231 - val_loss: 0.1189 - val_mae: 0.2597\n",
      "Epoch 48/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0855 - mae: 0.2235 - val_loss: 0.1071 - val_mae: 0.2405\n",
      "Epoch 49/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0816 - mae: 0.2173 - val_loss: 0.1108 - val_mae: 0.2462\n",
      "Epoch 50/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0834 - mae: 0.2203 - val_loss: 0.1100 - val_mae: 0.2434\n",
      "Epoch 51/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0812 - mae: 0.2161 - val_loss: 0.1136 - val_mae: 0.2515\n",
      "Epoch 52/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0805 - mae: 0.2165 - val_loss: 0.1087 - val_mae: 0.2442\n",
      "Epoch 53/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0792 - mae: 0.2151 - val_loss: 0.1078 - val_mae: 0.2391\n",
      "Epoch 54/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0792 - mae: 0.2145 - val_loss: 0.1136 - val_mae: 0.2499\n",
      "Epoch 55/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0794 - mae: 0.2154 - val_loss: 0.1134 - val_mae: 0.2533\n",
      "Epoch 56/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0782 - mae: 0.2139 - val_loss: 0.1175 - val_mae: 0.2572\n",
      "Epoch 57/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0762 - mae: 0.2118 - val_loss: 0.1108 - val_mae: 0.2439\n",
      "Epoch 58/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0789 - mae: 0.2144 - val_loss: 0.1108 - val_mae: 0.2452\n",
      "Epoch 59/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0782 - mae: 0.2139 - val_loss: 0.1135 - val_mae: 0.2478\n",
      "Epoch 60/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0763 - mae: 0.2107 - val_loss: 0.1109 - val_mae: 0.2450\n",
      "Epoch 61/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0751 - mae: 0.2085 - val_loss: 0.1073 - val_mae: 0.2403\n",
      "Epoch 62/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0759 - mae: 0.2101 - val_loss: 0.1150 - val_mae: 0.2495\n",
      "Epoch 63/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0763 - mae: 0.2105 - val_loss: 0.1075 - val_mae: 0.2387\n",
      "Epoch 64/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0750 - mae: 0.2089 - val_loss: 0.1086 - val_mae: 0.2385\n",
      "Epoch 65/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0755 - mae: 0.2095 - val_loss: 0.1189 - val_mae: 0.2574\n",
      "Epoch 66/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0728 - mae: 0.2064 - val_loss: 0.1195 - val_mae: 0.2585\n",
      "Epoch 67/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0717 - mae: 0.2050 - val_loss: 0.1134 - val_mae: 0.2510\n",
      "Epoch 68/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0749 - mae: 0.2084 - val_loss: 0.1125 - val_mae: 0.2468\n",
      "Epoch 69/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0726 - mae: 0.2055 - val_loss: 0.1139 - val_mae: 0.2509\n",
      "Epoch 70/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0728 - mae: 0.2060 - val_loss: 0.1140 - val_mae: 0.2480\n",
      "Epoch 71/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0727 - mae: 0.2065 - val_loss: 0.1111 - val_mae: 0.2447\n",
      "Epoch 72/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0729 - mae: 0.2058 - val_loss: 0.1107 - val_mae: 0.2455\n",
      "Epoch 73/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0714 - mae: 0.2035 - val_loss: 0.1135 - val_mae: 0.2473\n",
      "Epoch 74/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0699 - mae: 0.2022 - val_loss: 0.1092 - val_mae: 0.2416\n",
      "Epoch 75/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0709 - mae: 0.2030 - val_loss: 0.1123 - val_mae: 0.2466\n",
      "Epoch 76/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0706 - mae: 0.2018 - val_loss: 0.1094 - val_mae: 0.2400\n",
      "Epoch 77/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0702 - mae: 0.2015 - val_loss: 0.1108 - val_mae: 0.2451\n",
      "Epoch 78/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0696 - mae: 0.2008 - val_loss: 0.1144 - val_mae: 0.2516\n",
      "Epoch 79/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0697 - mae: 0.2013 - val_loss: 0.1098 - val_mae: 0.2412\n",
      "Epoch 80/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0687 - mae: 0.2005 - val_loss: 0.1106 - val_mae: 0.2447\n",
      "Epoch 81/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0699 - mae: 0.2024 - val_loss: 0.1108 - val_mae: 0.2449\n",
      "Epoch 82/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0707 - mae: 0.2026 - val_loss: 0.1074 - val_mae: 0.2398\n",
      "Epoch 83/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0687 - mae: 0.2004 - val_loss: 0.1117 - val_mae: 0.2453\n",
      "Epoch 84/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0701 - mae: 0.2010 - val_loss: 0.1087 - val_mae: 0.2423\n",
      "Epoch 85/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0687 - mae: 0.2001 - val_loss: 0.1115 - val_mae: 0.2436\n",
      "Epoch 86/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - loss: 0.0680 - mae: 0.1977 - val_loss: 0.1096 - val_mae: 0.2410\n",
      "Epoch 87/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0675 - mae: 0.1980 - val_loss: 0.1166 - val_mae: 0.2557\n",
      "Epoch 88/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0678 - mae: 0.1983 - val_loss: 0.1106 - val_mae: 0.2439\n",
      "Epoch 89/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0664 - mae: 0.1964 - val_loss: 0.1111 - val_mae: 0.2402\n",
      "Epoch 90/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.0671 - mae: 0.1966 - val_loss: 0.1105 - val_mae: 0.2422\n",
      "Epoch 91/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0663 - mae: 0.1951 - val_loss: 0.1079 - val_mae: 0.2390\n",
      "Epoch 92/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0672 - mae: 0.1976 - val_loss: 0.1105 - val_mae: 0.2443\n",
      "Epoch 93/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.0660 - mae: 0.1951 - val_loss: 0.1085 - val_mae: 0.2400\n",
      "Epoch 94/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0657 - mae: 0.1969 - val_loss: 0.1075 - val_mae: 0.2382\n",
      "Epoch 95/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0668 - mae: 0.1972 - val_loss: 0.1082 - val_mae: 0.2387\n",
      "Epoch 96/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - loss: 0.0665 - mae: 0.1970 - val_loss: 0.1079 - val_mae: 0.2373\n",
      "Epoch 97/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0672 - mae: 0.1972 - val_loss: 0.1081 - val_mae: 0.2381\n",
      "Epoch 98/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0672 - mae: 0.1969 - val_loss: 0.1134 - val_mae: 0.2448\n",
      "Epoch 99/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.0653 - mae: 0.1949 - val_loss: 0.1107 - val_mae: 0.2432\n",
      "Epoch 100/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0640 - mae: 0.1931 - val_loss: 0.1118 - val_mae: 0.2442\n"
     ]
    }
   ],
   "source": [
    "n_features_reg = X_train_reg_scaled.shape[1]\n",
    "\n",
    "model_reg_log = build_regression_model(n_features_reg)\n",
    "\n",
    "history_reg_log = model_reg_log.fit(\n",
    "    X_train_reg_scaled,\n",
    "    y_train_reg,\n",
    "    validation_data=(X_test_reg_scaled, y_test_reg),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3120bb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_log = model_reg_log.predict(X_test_reg_scaled).flatten()\n",
    "\n",
    "# Convertir log_price → price\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e7ca248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2385.367015378141\n",
      "RMSE: 48.84021923966088\n",
      "MAE: 32.31275649964698\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36dafc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID (reg): 614581612292287758\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\") \n",
    "experiment_reg = mlflow.set_experiment(\"airbnb-barcelona-reg-nn\")\n",
    "print(\"Experiment ID (reg):\", experiment_reg.experiment_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76fedeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_regression_model(\n",
    "    n_features,\n",
    "    n_hidden1=128,\n",
    "    n_hidden2=64,\n",
    "    n_hidden3=32,\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=1e-3\n",
    "):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(n_features,)),\n",
    "        layers.Dense(n_hidden1, activation=\"relu\"),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(n_hidden2, activation=\"relu\"),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(n_hidden3, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"linear\")  # regresión\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22f32267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def run_mlflow_reg(\n",
    "    n_hidden1=128,\n",
    "    n_hidden2=64,\n",
    "    n_hidden3=32,\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=1e-3,\n",
    "    epochs=80,\n",
    "    batch_size=32,\n",
    "    run_name=\"reg_base\",\n",
    "    experiment_id=None\n",
    "):\n",
    "    n_features_reg = X_train_reg_scaled.shape[1]\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n",
    "\n",
    "        # Log hiperparámetros\n",
    "        mlflow.log_param(\"n_hidden1\", n_hidden1)\n",
    "        mlflow.log_param(\"n_hidden2\", n_hidden2)\n",
    "        mlflow.log_param(\"n_hidden3\", n_hidden3)\n",
    "        mlflow.log_param(\"dropout_rate\", dropout_rate)\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "\n",
    "        # Construir modelo\n",
    "        model = build_regression_model(\n",
    "            n_features=n_features_reg,\n",
    "            n_hidden1=n_hidden1,\n",
    "            n_hidden2=n_hidden2,\n",
    "            n_hidden3=n_hidden3,\n",
    "            dropout_rate=dropout_rate,\n",
    "            learning_rate=learning_rate\n",
    "        )\n",
    "\n",
    "        # Entrenar\n",
    "        history = model.fit(\n",
    "            X_train_reg_scaled,\n",
    "            y_train_reg,\n",
    "            validation_data=(X_test_reg_scaled, y_test_reg),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Predicción en escala log\n",
    "        y_pred_log = model.predict(X_test_reg_scaled).flatten()\n",
    "\n",
    "        # Volver a precio real (euros)\n",
    "        y_true = np.expm1(y_test_reg)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "        # Métricas reales\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mean_price = y_true.mean()\n",
    "        rel_rmse = rmse / mean_price\n",
    "\n",
    "        # Log métricas\n",
    "        mlflow.log_metric(\"mse\", mse)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"rmse_over_mean_price\", rel_rmse)\n",
    "\n",
    "        # Guardar modelo\n",
    "        mlflow.keras.log_model(model, \"model\")\n",
    "\n",
    "        print(f\"Run '{run_name}' -> RMSE={rmse:.2f}, MAE={mae:.2f}, RMSE/Mean={rel_rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d42e5e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nconfigs_reg = [\\n    # ---- MODELOS PEQUEÑOS ----\\n    {\"run_name\": \"reg_small_1_500ep\", \"n_hidden1\": 32,  \"n_hidden2\": 16,  \"n_hidden3\": 8,   \"learning_rate\": 1e-3, \"dropout_rate\": 0.1, \"epochs\": 250,  \"batch_size\": 32},\\n\\n    {\"run_name\": \"reg_small_3_500ep\", \"n_hidden1\": 64,  \"n_hidden2\": 32,  \"n_hidden3\": 16,  \"learning_rate\": 1e-3, \"dropout_rate\": 0.2, \"epochs\": 250,  \"batch_size\": 64},\\n    {\"run_name\": \"reg_small_4_500ep\", \"n_hidden1\": 16,  \"n_hidden2\": 8,   \"n_hidden3\": 4,   \"learning_rate\": 1e-3, \"dropout_rate\": 0.0, \"epochs\": 250,  \"batch_size\": 32},\\n    {\"run_name\": \"reg_small_5_500ep\", \"n_hidden1\": 16,  \"n_hidden2\": 8,   \"n_hidden3\": 4,   \"learning_rate\": 5e-4, \"dropout_rate\": 0.1, \"epochs\": 250,  \"batch_size\": 32},\\n    # ---- MODELOS MEDIOS ----\\n    {\"run_name\": \"reg_medium_1_500ep\", \"n_hidden1\": 128, \"n_hidden2\": 64,  \"n_hidden3\": 32, \"learning_rate\": 1e-3, \"dropout_rate\": 0.2, \"epochs\": 250,  \"batch_size\": 32},\\n    {\"run_name\": \"reg_medium_2_500ep\", \"n_hidden1\": 128, \"n_hidden2\": 64,  \"n_hidden3\": 32, \"learning_rate\": 5e-4, \"dropout_rate\": 0.3, \"epochs\": 250,  \"batch_size\": 32},\\n    {\"run_name\": \"reg_medium_3_500ep\", \"n_hidden1\": 256, \"n_hidden2\": 128, \"n_hidden3\": 64, \"learning_rate\": 1e-3, \"dropout_rate\": 0.2, \"epochs\": 250,  \"batch_size\": 64},\\n    {\"run_name\": \"reg_medium_4_500ep\", \"n_hidden1\": 64,  \"n_hidden2\": 64,  \"n_hidden3\": 32, \"learning_rate\": 3e-4, \"dropout_rate\": 0.2, \"epochs\": 250,  \"batch_size\": 32},\\n    {\"run_name\": \"reg_medium_5_500ep\", \"n_hidden1\": 128, \"n_hidden2\": 64,  \"n_hidden3\": 32, \"learning_rate\": 1e-4, \"dropout_rate\": 0.2, \"epochs\": 250, \"batch_size\": 32},\\n\\n    # ---- MODELOS GRANDES ----\\n    {\"run_name\": \"reg_large_1_500ep\", \"n_hidden1\": 256, \"n_hidden2\": 128, \"n_hidden3\": 64,  \"learning_rate\": 1e-3, \"dropout_rate\": 0.3, \"epochs\": 250, \"batch_size\": 32},\\n    {\"run_name\": \"reg_large_2_500ep\", \"n_hidden1\": 256, \"n_hidden2\": 128, \"n_hidden3\": 64,  \"learning_rate\": 5e-4, \"dropout_rate\": 0.3, \"epochs\": 250, \"batch_size\": 32},\\n    {\"run_name\": \"reg_large_3_500ep\", \"n_hidden1\": 512, \"n_hidden2\": 256, \"n_hidden3\": 128, \"learning_rate\": 3e-4, \"dropout_rate\": 0.3, \"epochs\": 250, \"batch_size\": 64},\\n    {\"run_name\": \"reg_large_4_500ep\", \"n_hidden1\": 512, \"n_hidden2\": 256, \"n_hidden3\": 128, \"learning_rate\": 1e-4, \"dropout_rate\": 0.4, \"epochs\": 250, \"batch_size\": 32},\\n    {\"run_name\": \"reg_large_5_500ep\", \"n_hidden1\": 256, \"n_hidden2\": 128, \"n_hidden3\": 64,  \"learning_rate\": 2e-4, \"dropout_rate\": 0.3, \"epochs\": 250, \"batch_size\": 32},\\n\\n    # ---- VARIANDO LEARNING RATE ----\\n    {\"run_name\": \"reg_lr_1e_minus2_500ep\", \"n_hidden1\": 64,  \"n_hidden2\": 32,  \"n_hidden3\": 16, \"learning_rate\": 1e-2, \"dropout_rate\": 0.2, \"epochs\": 250,  \"batch_size\": 32},\\n    {\"run_name\": \"reg_lr_5e_minus3_500ep\", \"n_hidden1\": 64,  \"n_hidden2\": 32,  \"n_hidden3\": 16, \"learning_rate\": 5e-3, \"dropout_rate\": 0.2, \"epochs\": 250,  \"batch_size\": 32},\\n    {\"run_name\": \"reg_lr_3e_minus4_500ep\", \"n_hidden1\": 128, \"n_hidden2\": 64,  \"n_hidden3\": 32, \"learning_rate\": 3e-4, \"dropout_rate\": 0.2, \"epochs\": 250, \"batch_size\": 32},\\n    {\"run_name\": \"reg_lr_1e_minus4_500ep\", \"n_hidden1\": 128, \"n_hidden2\": 64,  \"n_hidden3\": 32, \"learning_rate\": 1e-4, \"dropout_rate\": 0.3, \"epochs\": 250, \"batch_size\": 32},\\n    {\"run_name\": \"reg_lr_5e_minus5_500ep\", \"n_hidden1\": 256, \"n_hidden2\": 128, \"n_hidden3\": 64, \"learning_rate\": 5e-5, \"dropout_rate\": 0.2, \"epochs\": 250, \"batch_size\": 32},\\n]\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#estas configs se demoran mucho\n",
    "\"\"\"\n",
    "configs_reg = [\n",
    "    # ---- MODELOS PEQUEÑOS ----\n",
    "    {\"run_name\": \"reg_small_1_500ep\", \"n_hidden1\": 32,  \"n_hidden2\": 16,  \"n_hidden3\": 8,   \"learning_rate\": 1e-3, \"dropout_rate\": 0.1, \"epochs\": 250,  \"batch_size\": 32},\n",
    "\n",
    "    {\"run_name\": \"reg_small_3_500ep\", \"n_hidden1\": 64,  \"n_hidden2\": 32,  \"n_hidden3\": 16,  \"learning_rate\": 1e-3, \"dropout_rate\": 0.2, \"epochs\": 250,  \"batch_size\": 64},\n",
    "    {\"run_name\": \"reg_small_4_500ep\", \"n_hidden1\": 16,  \"n_hidden2\": 8,   \"n_hidden3\": 4,   \"learning_rate\": 1e-3, \"dropout_rate\": 0.0, \"epochs\": 250,  \"batch_size\": 32},\n",
    "    {\"run_name\": \"reg_small_5_500ep\", \"n_hidden1\": 16,  \"n_hidden2\": 8,   \"n_hidden3\": 4,   \"learning_rate\": 5e-4, \"dropout_rate\": 0.1, \"epochs\": 250,  \"batch_size\": 32},\n",
    "    # ---- MODELOS MEDIOS ----\n",
    "    {\"run_name\": \"reg_medium_1_500ep\", \"n_hidden1\": 128, \"n_hidden2\": 64,  \"n_hidden3\": 32, \"learning_rate\": 1e-3, \"dropout_rate\": 0.2, \"epochs\": 250,  \"batch_size\": 32},\n",
    "    {\"run_name\": \"reg_medium_2_500ep\", \"n_hidden1\": 128, \"n_hidden2\": 64,  \"n_hidden3\": 32, \"learning_rate\": 5e-4, \"dropout_rate\": 0.3, \"epochs\": 250,  \"batch_size\": 32},\n",
    "    {\"run_name\": \"reg_medium_3_500ep\", \"n_hidden1\": 256, \"n_hidden2\": 128, \"n_hidden3\": 64, \"learning_rate\": 1e-3, \"dropout_rate\": 0.2, \"epochs\": 250,  \"batch_size\": 64},\n",
    "    {\"run_name\": \"reg_medium_4_500ep\", \"n_hidden1\": 64,  \"n_hidden2\": 64,  \"n_hidden3\": 32, \"learning_rate\": 3e-4, \"dropout_rate\": 0.2, \"epochs\": 250,  \"batch_size\": 32},\n",
    "    {\"run_name\": \"reg_medium_5_500ep\", \"n_hidden1\": 128, \"n_hidden2\": 64,  \"n_hidden3\": 32, \"learning_rate\": 1e-4, \"dropout_rate\": 0.2, \"epochs\": 250, \"batch_size\": 32},\n",
    "\n",
    "    # ---- MODELOS GRANDES ----\n",
    "    {\"run_name\": \"reg_large_1_500ep\", \"n_hidden1\": 256, \"n_hidden2\": 128, \"n_hidden3\": 64,  \"learning_rate\": 1e-3, \"dropout_rate\": 0.3, \"epochs\": 250, \"batch_size\": 32},\n",
    "    {\"run_name\": \"reg_large_2_500ep\", \"n_hidden1\": 256, \"n_hidden2\": 128, \"n_hidden3\": 64,  \"learning_rate\": 5e-4, \"dropout_rate\": 0.3, \"epochs\": 250, \"batch_size\": 32},\n",
    "    {\"run_name\": \"reg_large_3_500ep\", \"n_hidden1\": 512, \"n_hidden2\": 256, \"n_hidden3\": 128, \"learning_rate\": 3e-4, \"dropout_rate\": 0.3, \"epochs\": 250, \"batch_size\": 64},\n",
    "    {\"run_name\": \"reg_large_4_500ep\", \"n_hidden1\": 512, \"n_hidden2\": 256, \"n_hidden3\": 128, \"learning_rate\": 1e-4, \"dropout_rate\": 0.4, \"epochs\": 250, \"batch_size\": 32},\n",
    "    {\"run_name\": \"reg_large_5_500ep\", \"n_hidden1\": 256, \"n_hidden2\": 128, \"n_hidden3\": 64,  \"learning_rate\": 2e-4, \"dropout_rate\": 0.3, \"epochs\": 250, \"batch_size\": 32},\n",
    "\n",
    "    # ---- VARIANDO LEARNING RATE ----\n",
    "    {\"run_name\": \"reg_lr_1e_minus2_500ep\", \"n_hidden1\": 64,  \"n_hidden2\": 32,  \"n_hidden3\": 16, \"learning_rate\": 1e-2, \"dropout_rate\": 0.2, \"epochs\": 250,  \"batch_size\": 32},\n",
    "    {\"run_name\": \"reg_lr_5e_minus3_500ep\", \"n_hidden1\": 64,  \"n_hidden2\": 32,  \"n_hidden3\": 16, \"learning_rate\": 5e-3, \"dropout_rate\": 0.2, \"epochs\": 250,  \"batch_size\": 32},\n",
    "    {\"run_name\": \"reg_lr_3e_minus4_500ep\", \"n_hidden1\": 128, \"n_hidden2\": 64,  \"n_hidden3\": 32, \"learning_rate\": 3e-4, \"dropout_rate\": 0.2, \"epochs\": 250, \"batch_size\": 32},\n",
    "    {\"run_name\": \"reg_lr_1e_minus4_500ep\", \"n_hidden1\": 128, \"n_hidden2\": 64,  \"n_hidden3\": 32, \"learning_rate\": 1e-4, \"dropout_rate\": 0.3, \"epochs\": 250, \"batch_size\": 32},\n",
    "    {\"run_name\": \"reg_lr_5e_minus5_500ep\", \"n_hidden1\": 256, \"n_hidden2\": 128, \"n_hidden3\": 64, \"learning_rate\": 5e-5, \"dropout_rate\": 0.2, \"epochs\": 250, \"batch_size\": 32},\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0d942c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor cfg in configs_reg:\\n    print(f\"=== Corriendo {cfg[\\'run_name\\']} ===\")\\n    run_mlflow_reg(**cfg, experiment_id=experiment_reg.experiment_id)\\n    print()\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for cfg in configs_reg:\n",
    "    print(f\"=== Corriendo {cfg['run_name']} ===\")\n",
    "    run_mlflow_reg(**cfg, experiment_id=experiment_reg.experiment_id)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee31f5",
   "metadata": {},
   "source": [
    "Haremos el mismo modelo pero con ELU porque según chat es mejor :D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbf7c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_regression_model_elu(\n",
    "    n_features,\n",
    "    n_hidden1=128,\n",
    "    n_hidden2=64,\n",
    "    n_hidden3=32,\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=1e-3\n",
    "):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(n_features,)),\n",
    "        layers.Dense(n_hidden1, activation=\"elu\"),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(n_hidden2, activation=\"elu\"),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(n_hidden3, activation=\"elu\"),\n",
    "        layers.Dense(1, activation=\"linear\")  # regresión real\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed885e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 2.0110 - mae: 0.9970 - val_loss: 0.2467 - val_mae: 0.3893\n",
      "Epoch 2/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3729 - mae: 0.4713 - val_loss: 0.1768 - val_mae: 0.3246\n",
      "Epoch 3/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.2306 - mae: 0.3744 - val_loss: 0.1803 - val_mae: 0.3341\n",
      "Epoch 4/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.1893 - mae: 0.3388 - val_loss: 0.1603 - val_mae: 0.3098\n",
      "Epoch 5/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.1750 - mae: 0.3251 - val_loss: 0.1515 - val_mae: 0.2958\n",
      "Epoch 6/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1696 - mae: 0.3182 - val_loss: 0.1528 - val_mae: 0.2988\n",
      "Epoch 7/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1616 - mae: 0.3107 - val_loss: 0.1520 - val_mae: 0.2962\n",
      "Epoch 8/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1586 - mae: 0.3066 - val_loss: 0.1452 - val_mae: 0.2908\n",
      "Epoch 9/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1541 - mae: 0.3023 - val_loss: 0.1479 - val_mae: 0.2953\n",
      "Epoch 10/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1495 - mae: 0.2978 - val_loss: 0.1426 - val_mae: 0.2872\n",
      "Epoch 11/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1465 - mae: 0.2945 - val_loss: 0.1420 - val_mae: 0.2862\n",
      "Epoch 12/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1449 - mae: 0.2930 - val_loss: 0.1457 - val_mae: 0.2871\n",
      "Epoch 13/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1422 - mae: 0.2887 - val_loss: 0.1366 - val_mae: 0.2777\n",
      "Epoch 14/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1417 - mae: 0.2888 - val_loss: 0.1361 - val_mae: 0.2771\n",
      "Epoch 15/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1386 - mae: 0.2842 - val_loss: 0.1366 - val_mae: 0.2766\n",
      "Epoch 16/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1375 - mae: 0.2846 - val_loss: 0.1494 - val_mae: 0.2996\n",
      "Epoch 17/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1365 - mae: 0.2827 - val_loss: 0.1347 - val_mae: 0.2741\n",
      "Epoch 18/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1344 - mae: 0.2808 - val_loss: 0.1357 - val_mae: 0.2748\n",
      "Epoch 19/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1330 - mae: 0.2793 - val_loss: 0.1346 - val_mae: 0.2759\n",
      "Epoch 20/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1309 - mae: 0.2764 - val_loss: 0.1342 - val_mae: 0.2777\n",
      "Epoch 21/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1289 - mae: 0.2741 - val_loss: 0.1363 - val_mae: 0.2790\n",
      "Epoch 22/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1289 - mae: 0.2743 - val_loss: 0.1332 - val_mae: 0.2713\n",
      "Epoch 23/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1277 - mae: 0.2733 - val_loss: 0.1299 - val_mae: 0.2690\n",
      "Epoch 24/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1258 - mae: 0.2707 - val_loss: 0.1310 - val_mae: 0.2703\n",
      "Epoch 25/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1243 - mae: 0.2675 - val_loss: 0.1270 - val_mae: 0.2665\n",
      "Epoch 26/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1221 - mae: 0.2654 - val_loss: 0.1256 - val_mae: 0.2627\n",
      "Epoch 27/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1218 - mae: 0.2662 - val_loss: 0.1254 - val_mae: 0.2653\n",
      "Epoch 28/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1222 - mae: 0.2660 - val_loss: 0.1269 - val_mae: 0.2637\n",
      "Epoch 29/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1201 - mae: 0.2638 - val_loss: 0.1242 - val_mae: 0.2604\n",
      "Epoch 30/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1189 - mae: 0.2626 - val_loss: 0.1278 - val_mae: 0.2691\n",
      "Epoch 31/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1184 - mae: 0.2625 - val_loss: 0.1218 - val_mae: 0.2583\n",
      "Epoch 32/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1176 - mae: 0.2611 - val_loss: 0.1221 - val_mae: 0.2579\n",
      "Epoch 33/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1156 - mae: 0.2581 - val_loss: 0.1247 - val_mae: 0.2595\n",
      "Epoch 34/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1165 - mae: 0.2591 - val_loss: 0.1205 - val_mae: 0.2587\n",
      "Epoch 35/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1150 - mae: 0.2579 - val_loss: 0.1284 - val_mae: 0.2702\n",
      "Epoch 36/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1210 - mae: 0.2604 - val_loss: 0.1233 - val_mae: 0.2620\n",
      "Epoch 37/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1155 - mae: 0.2587 - val_loss: 0.1219 - val_mae: 0.2608\n",
      "Epoch 38/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1149 - mae: 0.2576 - val_loss: 0.1239 - val_mae: 0.2645\n",
      "Epoch 39/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1109 - mae: 0.2535 - val_loss: 0.1253 - val_mae: 0.2651\n",
      "Epoch 40/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1114 - mae: 0.2540 - val_loss: 0.1225 - val_mae: 0.2627\n",
      "Epoch 41/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.1111 - mae: 0.2525 - val_loss: 0.1177 - val_mae: 0.2531\n",
      "Epoch 42/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1116 - mae: 0.2543 - val_loss: 0.1183 - val_mae: 0.2543\n",
      "Epoch 43/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.1087 - mae: 0.2520 - val_loss: 0.1207 - val_mae: 0.2572\n",
      "Epoch 44/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.1089 - mae: 0.2525 - val_loss: 0.1154 - val_mae: 0.2522\n",
      "Epoch 45/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1079 - mae: 0.2501 - val_loss: 0.1142 - val_mae: 0.2513\n",
      "Epoch 46/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1084 - mae: 0.2501 - val_loss: 0.1159 - val_mae: 0.2526\n",
      "Epoch 47/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1068 - mae: 0.2478 - val_loss: 0.1143 - val_mae: 0.2528\n",
      "Epoch 48/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1100 - mae: 0.2514 - val_loss: 0.1145 - val_mae: 0.2509\n",
      "Epoch 49/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1057 - mae: 0.2472 - val_loss: 0.1145 - val_mae: 0.2517\n",
      "Epoch 50/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1056 - mae: 0.2486 - val_loss: 0.1128 - val_mae: 0.2479\n",
      "Epoch 51/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1035 - mae: 0.2448 - val_loss: 0.1174 - val_mae: 0.2572\n",
      "Epoch 52/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1032 - mae: 0.2445 - val_loss: 0.1147 - val_mae: 0.2515\n",
      "Epoch 53/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1037 - mae: 0.2442 - val_loss: 0.1130 - val_mae: 0.2491\n",
      "Epoch 54/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1029 - mae: 0.2432 - val_loss: 0.1146 - val_mae: 0.2520\n",
      "Epoch 55/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1021 - mae: 0.2443 - val_loss: 0.1123 - val_mae: 0.2485\n",
      "Epoch 56/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1022 - mae: 0.2437 - val_loss: 0.1162 - val_mae: 0.2550\n",
      "Epoch 57/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.1011 - mae: 0.2419 - val_loss: 0.1126 - val_mae: 0.2505\n",
      "Epoch 58/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.1012 - mae: 0.2433 - val_loss: 0.1134 - val_mae: 0.2477\n",
      "Epoch 59/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1011 - mae: 0.2424 - val_loss: 0.1116 - val_mae: 0.2485\n",
      "Epoch 60/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1002 - mae: 0.2426 - val_loss: 0.1110 - val_mae: 0.2473\n",
      "Epoch 61/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0998 - mae: 0.2408 - val_loss: 0.1184 - val_mae: 0.2542\n",
      "Epoch 62/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0990 - mae: 0.2392 - val_loss: 0.1154 - val_mae: 0.2520\n",
      "Epoch 63/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1006 - mae: 0.2411 - val_loss: 0.1135 - val_mae: 0.2491\n",
      "Epoch 64/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0988 - mae: 0.2386 - val_loss: 0.1100 - val_mae: 0.2466\n",
      "Epoch 65/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0982 - mae: 0.2392 - val_loss: 0.1109 - val_mae: 0.2467\n",
      "Epoch 66/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0974 - mae: 0.2367 - val_loss: 0.1084 - val_mae: 0.2443\n",
      "Epoch 67/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0957 - mae: 0.2344 - val_loss: 0.1076 - val_mae: 0.2434\n",
      "Epoch 68/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0964 - mae: 0.2363 - val_loss: 0.1151 - val_mae: 0.2519\n",
      "Epoch 69/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0984 - mae: 0.2396 - val_loss: 0.1129 - val_mae: 0.2499\n",
      "Epoch 70/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0965 - mae: 0.2373 - val_loss: 0.1084 - val_mae: 0.2449\n",
      "Epoch 71/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1007 - mae: 0.2400 - val_loss: 0.1127 - val_mae: 0.2502\n",
      "Epoch 72/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0953 - mae: 0.2345 - val_loss: 0.1078 - val_mae: 0.2433\n",
      "Epoch 73/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0960 - mae: 0.2356 - val_loss: 0.1091 - val_mae: 0.2442\n",
      "Epoch 74/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0944 - mae: 0.2341 - val_loss: 0.1079 - val_mae: 0.2412\n",
      "Epoch 75/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0930 - mae: 0.2321 - val_loss: 0.1072 - val_mae: 0.2412\n",
      "Epoch 76/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0935 - mae: 0.2328 - val_loss: 0.1079 - val_mae: 0.2455\n",
      "Epoch 77/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0940 - mae: 0.2332 - val_loss: 0.1070 - val_mae: 0.2411\n",
      "Epoch 78/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0945 - mae: 0.2325 - val_loss: 0.1099 - val_mae: 0.2454\n",
      "Epoch 79/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0941 - mae: 0.2346 - val_loss: 0.1073 - val_mae: 0.2412\n",
      "Epoch 80/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0938 - mae: 0.2340 - val_loss: 0.1077 - val_mae: 0.2426\n",
      "Epoch 81/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0926 - mae: 0.2317 - val_loss: 0.1075 - val_mae: 0.2423\n",
      "Epoch 82/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0920 - mae: 0.2315 - val_loss: 0.1090 - val_mae: 0.2444\n",
      "Epoch 83/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0954 - mae: 0.2347 - val_loss: 0.1093 - val_mae: 0.2435\n",
      "Epoch 84/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0936 - mae: 0.2330 - val_loss: 0.1055 - val_mae: 0.2383\n",
      "Epoch 85/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0911 - mae: 0.2303 - val_loss: 0.1067 - val_mae: 0.2410\n",
      "Epoch 86/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0923 - mae: 0.2310 - val_loss: 0.1068 - val_mae: 0.2416\n",
      "Epoch 87/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0893 - mae: 0.2267 - val_loss: 0.1100 - val_mae: 0.2458\n",
      "Epoch 88/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0909 - mae: 0.2305 - val_loss: 0.1092 - val_mae: 0.2414\n",
      "Epoch 89/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0901 - mae: 0.2291 - val_loss: 0.1053 - val_mae: 0.2389\n",
      "Epoch 90/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0906 - mae: 0.2288 - val_loss: 0.1074 - val_mae: 0.2409\n",
      "Epoch 91/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0894 - mae: 0.2278 - val_loss: 0.1084 - val_mae: 0.2425\n",
      "Epoch 92/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0893 - mae: 0.2278 - val_loss: 0.1061 - val_mae: 0.2408\n",
      "Epoch 93/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0901 - mae: 0.2285 - val_loss: 0.1054 - val_mae: 0.2379\n",
      "Epoch 94/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0882 - mae: 0.2266 - val_loss: 0.1066 - val_mae: 0.2389\n",
      "Epoch 95/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0885 - mae: 0.2278 - val_loss: 0.1060 - val_mae: 0.2384\n",
      "Epoch 96/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0875 - mae: 0.2259 - val_loss: 0.1073 - val_mae: 0.2403\n",
      "Epoch 97/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0891 - mae: 0.2273 - val_loss: 0.1075 - val_mae: 0.2435\n",
      "Epoch 98/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0887 - mae: 0.2268 - val_loss: 0.1058 - val_mae: 0.2382\n",
      "Epoch 99/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0893 - mae: 0.2277 - val_loss: 0.1054 - val_mae: 0.2381\n",
      "Epoch 100/100\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0873 - mae: 0.2255 - val_loss: 0.1062 - val_mae: 0.2388\n"
     ]
    }
   ],
   "source": [
    "n_features_reg = X_train_reg_scaled.shape[1]\n",
    "\n",
    "model_reg_elu = build_regression_model_elu(n_features_reg)\n",
    "\n",
    "history_reg_elu = model_reg_elu.fit(\n",
    "    X_train_reg_scaled,\n",
    "    y_train_reg,\n",
    "    validation_data=(X_test_reg_scaled, y_test_reg),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ddbb3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "MSE: 2223.43232598581\n",
      "RMSE: 47.15328542091007\n",
      "MAE: 31.862441858781438\n",
      "RMSE / mean_price: 0.32785050632910856\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "y_pred_log = model_reg_elu.predict(X_test_reg_scaled).flatten()\n",
    "\n",
    "# Volver al precio real\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test_reg)\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "mean_price = y_true.mean()\n",
    "rel_rmse = rmse / mean_price\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE / mean_price:\", rel_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "304b8e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: (13415, 49)\n",
      "Después: (13282, 49)\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jo_ga\\AppData\\Local\\Temp\\ipykernel_19388\\1489508282.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filt[\"log_price\"] = np.log1p(df_filt[\"price\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.9751 - mae: 0.9792 - val_loss: 0.2079 - val_mae: 0.3504\n",
      "Epoch 2/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3328 - mae: 0.4518 - val_loss: 0.1715 - val_mae: 0.3200\n",
      "Epoch 3/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2190 - mae: 0.3671 - val_loss: 0.1757 - val_mae: 0.3226\n",
      "Epoch 4/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1895 - mae: 0.3411 - val_loss: 0.1522 - val_mae: 0.2993\n",
      "Epoch 5/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1715 - mae: 0.3222 - val_loss: 0.1495 - val_mae: 0.2924\n",
      "Epoch 6/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1673 - mae: 0.3165 - val_loss: 0.1669 - val_mae: 0.3153\n",
      "Epoch 7/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1616 - mae: 0.3123 - val_loss: 0.1457 - val_mae: 0.2858\n",
      "Epoch 8/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1568 - mae: 0.3061 - val_loss: 0.1513 - val_mae: 0.2958\n",
      "Epoch 9/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1542 - mae: 0.3032 - val_loss: 0.1351 - val_mae: 0.2774\n",
      "Epoch 10/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1476 - mae: 0.2973 - val_loss: 0.1331 - val_mae: 0.2748\n",
      "Epoch 11/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1463 - mae: 0.2953 - val_loss: 0.1403 - val_mae: 0.2768\n",
      "Epoch 12/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1446 - mae: 0.2926 - val_loss: 0.1360 - val_mae: 0.2781\n",
      "Epoch 13/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1425 - mae: 0.2911 - val_loss: 0.1608 - val_mae: 0.3138\n",
      "Epoch 14/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1419 - mae: 0.2902 - val_loss: 0.1400 - val_mae: 0.2872\n",
      "Epoch 15/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1384 - mae: 0.2864 - val_loss: 0.1284 - val_mae: 0.2701\n",
      "Epoch 16/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1348 - mae: 0.2828 - val_loss: 0.1382 - val_mae: 0.2759\n",
      "Epoch 17/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1350 - mae: 0.2812 - val_loss: 0.1243 - val_mae: 0.2641\n",
      "Epoch 18/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1321 - mae: 0.2786 - val_loss: 0.1275 - val_mae: 0.2688\n",
      "Epoch 19/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1384 - mae: 0.2820 - val_loss: 0.1257 - val_mae: 0.2667\n",
      "Epoch 20/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1298 - mae: 0.2754 - val_loss: 0.1221 - val_mae: 0.2622\n",
      "Epoch 21/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1285 - mae: 0.2745 - val_loss: 0.1293 - val_mae: 0.2666\n",
      "Epoch 22/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1273 - mae: 0.2743 - val_loss: 0.1192 - val_mae: 0.2581\n",
      "Epoch 23/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1250 - mae: 0.2716 - val_loss: 0.1233 - val_mae: 0.2614\n",
      "Epoch 24/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1224 - mae: 0.2685 - val_loss: 0.1198 - val_mae: 0.2549\n",
      "Epoch 25/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1215 - mae: 0.2658 - val_loss: 0.1234 - val_mae: 0.2646\n",
      "Epoch 26/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1229 - mae: 0.2687 - val_loss: 0.1207 - val_mae: 0.2577\n",
      "Epoch 27/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1214 - mae: 0.2660 - val_loss: 0.1216 - val_mae: 0.2573\n",
      "Epoch 28/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1212 - mae: 0.2660 - val_loss: 0.1158 - val_mae: 0.2508\n",
      "Epoch 29/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1196 - mae: 0.2644 - val_loss: 0.1188 - val_mae: 0.2559\n",
      "Epoch 30/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1182 - mae: 0.2615 - val_loss: 0.1167 - val_mae: 0.2519\n",
      "Epoch 31/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1165 - mae: 0.2610 - val_loss: 0.1147 - val_mae: 0.2512\n",
      "Epoch 32/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1153 - mae: 0.2599 - val_loss: 0.1184 - val_mae: 0.2584\n",
      "Epoch 33/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1155 - mae: 0.2593 - val_loss: 0.1186 - val_mae: 0.2538\n",
      "Epoch 34/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1140 - mae: 0.2580 - val_loss: 0.1178 - val_mae: 0.2545\n",
      "Epoch 35/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1129 - mae: 0.2558 - val_loss: 0.1149 - val_mae: 0.2498\n",
      "Epoch 36/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1108 - mae: 0.2533 - val_loss: 0.1152 - val_mae: 0.2507\n",
      "Epoch 37/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1111 - mae: 0.2545 - val_loss: 0.1143 - val_mae: 0.2508\n",
      "Epoch 38/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1117 - mae: 0.2554 - val_loss: 0.1149 - val_mae: 0.2495\n",
      "Epoch 39/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1089 - mae: 0.2519 - val_loss: 0.1148 - val_mae: 0.2460\n",
      "Epoch 40/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1084 - mae: 0.2503 - val_loss: 0.1125 - val_mae: 0.2464\n",
      "Epoch 41/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1074 - mae: 0.2502 - val_loss: 0.1173 - val_mae: 0.2536\n",
      "Epoch 42/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1078 - mae: 0.2511 - val_loss: 0.1146 - val_mae: 0.2474\n",
      "Epoch 43/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1068 - mae: 0.2489 - val_loss: 0.1138 - val_mae: 0.2492\n",
      "Epoch 44/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1057 - mae: 0.2469 - val_loss: 0.1107 - val_mae: 0.2448\n",
      "Epoch 45/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1072 - mae: 0.2492 - val_loss: 0.1099 - val_mae: 0.2456\n",
      "Epoch 46/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1047 - mae: 0.2466 - val_loss: 0.1114 - val_mae: 0.2460\n",
      "Epoch 47/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1034 - mae: 0.2457 - val_loss: 0.1103 - val_mae: 0.2444\n",
      "Epoch 48/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1026 - mae: 0.2439 - val_loss: 0.1099 - val_mae: 0.2460\n",
      "Epoch 49/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1036 - mae: 0.2458 - val_loss: 0.1079 - val_mae: 0.2429\n",
      "Epoch 50/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1058 - mae: 0.2463 - val_loss: 0.1106 - val_mae: 0.2458\n",
      "Epoch 51/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1037 - mae: 0.2444 - val_loss: 0.1088 - val_mae: 0.2430\n",
      "Epoch 52/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1036 - mae: 0.2453 - val_loss: 0.1117 - val_mae: 0.2458\n",
      "Epoch 53/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1012 - mae: 0.2424 - val_loss: 0.1104 - val_mae: 0.2453\n",
      "Epoch 54/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1023 - mae: 0.2433 - val_loss: 0.1110 - val_mae: 0.2462\n",
      "Epoch 55/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0992 - mae: 0.2395 - val_loss: 0.1103 - val_mae: 0.2453\n",
      "Epoch 56/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1009 - mae: 0.2418 - val_loss: 0.1071 - val_mae: 0.2416\n",
      "Epoch 57/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1007 - mae: 0.2407 - val_loss: 0.1146 - val_mae: 0.2511\n",
      "Epoch 58/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0985 - mae: 0.2389 - val_loss: 0.1085 - val_mae: 0.2402\n",
      "Epoch 59/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1039 - mae: 0.2422 - val_loss: 0.1055 - val_mae: 0.2384\n",
      "Epoch 60/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0985 - mae: 0.2372 - val_loss: 0.1070 - val_mae: 0.2432\n",
      "Epoch 61/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1001 - mae: 0.2420 - val_loss: 0.1069 - val_mae: 0.2414\n",
      "Epoch 62/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0978 - mae: 0.2385 - val_loss: 0.1063 - val_mae: 0.2396\n",
      "Epoch 63/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0974 - mae: 0.2380 - val_loss: 0.1133 - val_mae: 0.2470\n",
      "Epoch 64/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0966 - mae: 0.2368 - val_loss: 0.1044 - val_mae: 0.2365\n",
      "Epoch 65/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0967 - mae: 0.2375 - val_loss: 0.1100 - val_mae: 0.2425\n",
      "Epoch 66/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0971 - mae: 0.2375 - val_loss: 0.1071 - val_mae: 0.2397\n",
      "Epoch 67/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0947 - mae: 0.2347 - val_loss: 0.1088 - val_mae: 0.2426\n",
      "Epoch 68/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0962 - mae: 0.2364 - val_loss: 0.1128 - val_mae: 0.2453\n",
      "Epoch 69/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0996 - mae: 0.2395 - val_loss: 0.1091 - val_mae: 0.2437\n",
      "Epoch 70/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0946 - mae: 0.2332 - val_loss: 0.1056 - val_mae: 0.2371\n",
      "Epoch 71/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0936 - mae: 0.2338 - val_loss: 0.1082 - val_mae: 0.2418\n",
      "Epoch 72/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0939 - mae: 0.2341 - val_loss: 0.1031 - val_mae: 0.2339\n",
      "Epoch 73/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0934 - mae: 0.2320 - val_loss: 0.1099 - val_mae: 0.2427\n",
      "Epoch 74/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0938 - mae: 0.2337 - val_loss: 0.1048 - val_mae: 0.2384\n",
      "Epoch 75/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0926 - mae: 0.2316 - val_loss: 0.1045 - val_mae: 0.2363\n",
      "Epoch 76/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0932 - mae: 0.2317 - val_loss: 0.1038 - val_mae: 0.2339\n",
      "Epoch 77/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0917 - mae: 0.2308 - val_loss: 0.1053 - val_mae: 0.2346\n",
      "Epoch 78/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0939 - mae: 0.2346 - val_loss: 0.1030 - val_mae: 0.2348\n",
      "Epoch 79/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0907 - mae: 0.2290 - val_loss: 0.1038 - val_mae: 0.2349\n",
      "Epoch 80/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0921 - mae: 0.2311 - val_loss: 0.1040 - val_mae: 0.2348\n",
      "Epoch 81/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0920 - mae: 0.2313 - val_loss: 0.1029 - val_mae: 0.2338\n",
      "Epoch 82/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0949 - mae: 0.2333 - val_loss: 0.1065 - val_mae: 0.2375\n",
      "Epoch 83/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0943 - mae: 0.2338 - val_loss: 0.1068 - val_mae: 0.2374\n",
      "Epoch 84/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0913 - mae: 0.2301 - val_loss: 0.1044 - val_mae: 0.2379\n",
      "Epoch 85/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0912 - mae: 0.2308 - val_loss: 0.1017 - val_mae: 0.2321\n",
      "Epoch 86/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0903 - mae: 0.2288 - val_loss: 0.1025 - val_mae: 0.2340\n",
      "Epoch 87/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0900 - mae: 0.2298 - val_loss: 0.1162 - val_mae: 0.2516\n",
      "Epoch 88/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0931 - mae: 0.2346 - val_loss: 0.1030 - val_mae: 0.2330\n",
      "Epoch 89/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0892 - mae: 0.2274 - val_loss: 0.1065 - val_mae: 0.2414\n",
      "Epoch 90/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0892 - mae: 0.2275 - val_loss: 0.1031 - val_mae: 0.2342\n",
      "Epoch 91/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0888 - mae: 0.2270 - val_loss: 0.1010 - val_mae: 0.2348\n",
      "Epoch 92/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0908 - mae: 0.2304 - val_loss: 0.1037 - val_mae: 0.2345\n",
      "Epoch 93/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0890 - mae: 0.2286 - val_loss: 0.1012 - val_mae: 0.2299\n",
      "Epoch 94/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0875 - mae: 0.2267 - val_loss: 0.1038 - val_mae: 0.2359\n",
      "Epoch 95/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0865 - mae: 0.2243 - val_loss: 0.1009 - val_mae: 0.2299\n",
      "Epoch 96/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0882 - mae: 0.2255 - val_loss: 0.1017 - val_mae: 0.2309\n",
      "Epoch 97/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0882 - mae: 0.2266 - val_loss: 0.1032 - val_mae: 0.2335\n",
      "Epoch 98/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0919 - mae: 0.2287 - val_loss: 0.1049 - val_mae: 0.2377\n",
      "Epoch 99/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0865 - mae: 0.2245 - val_loss: 0.1122 - val_mae: 0.2446\n",
      "Epoch 100/100\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0866 - mae: 0.2244 - val_loss: 0.1027 - val_mae: 0.2368\n"
     ]
    }
   ],
   "source": [
    "p99 = df[\"price\"].quantile(0.99)\n",
    "print(\"Antes:\", df.shape)\n",
    "df_filt = df[df[\"price\"] <= p99]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Después:\", df_filt.shape) \n",
    "df_filt[\"log_price\"] = np.log1p(df_filt[\"price\"])\n",
    "\n",
    "\n",
    "\n",
    "cols_excluir_reg = [\n",
    "    \"price\",\n",
    "    \"log_price\",\n",
    "    \"estimated_revenue_l365d\"  # si existe\n",
    "]\n",
    "\n",
    "\n",
    "y_reg = df_filt[\"log_price\"].values\n",
    "\n",
    "X_reg = df_filt.drop(columns=cols_excluir_reg)\n",
    "X_reg = X_reg.select_dtypes(include=[\"number\"])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "n_features_reg = X_train_reg_scaled.shape[1]\n",
    "model_reg_elu = build_regression_model_elu(n_features_reg)\n",
    "\n",
    "history_reg_elu = model_reg_elu.fit(\n",
    "    X_train_reg_scaled,\n",
    "    y_train_reg,\n",
    "    validation_data=(X_test_reg_scaled, y_test_reg),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ae1624e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "MSE: 2333.4818210825288\n",
      "RMSE: 48.30612612373847\n",
      "MAE: 31.882638967167313\n",
      "RMSE / mean_price: 0.33701299511288907\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "y_pred_log = model_reg_elu.predict(X_test_reg_scaled).flatten()\n",
    "\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test_reg)\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "mean_price = y_true.mean()\n",
    "rel_rmse = rmse / mean_price\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE / mean_price:\", rel_rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
